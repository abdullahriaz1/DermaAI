{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c915d368",
      "metadata": {
        "id": "c915d368"
      },
      "source": [
        "# Setup and Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffeeb0d5",
      "metadata": {
        "id": "ffeeb0d5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q datasets\n",
        "!pip install matplotlib\n",
        "!pip install -U transformers\n",
        "!pip install scikit-learn pillow torchvision opencv-python\n",
        "!pip install tensorboardX\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install -U accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In Google Colab, use /content folder for uploading lump.zip"
      ],
      "metadata": {
        "id": "BRIOi_Dsi5Q0"
      },
      "id": "BRIOi_Dsi5Q0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062f2e88",
      "metadata": {
        "id": "062f2e88"
      },
      "outputs": [],
      "source": [
        "!unzip lump.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0107a42",
      "metadata": {
        "id": "e0107a42"
      },
      "source": [
        "# Data Loading and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bfc533",
      "metadata": {
        "id": "56bfc533"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load and prepare dataset\n",
        "csv_file = 'lump/merged.csv'  # Modify this path as needed\n",
        "df = pd.read_csv(csv_file)\n",
        "df.columns = df.columns.str.strip()\n",
        "print(\"Columns in the DataFrame:\", df.columns)\n",
        "\n",
        "# Define classes for acne classification\n",
        "class_labels = ['blackheads', 'dark_spot', 'nodules', 'papules', 'pustules', 'whiteheads']\n",
        "\n",
        "# Convert DataFrame to Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "ds = dataset.train_test_split(test_size=0.3)  # 70% train, 30% test\n",
        "ds_test = ds['test'].train_test_split(test_size=0.5)  # 30% test --> 15% valid, 15% test\n",
        "\n",
        "prepared_ds = DatasetDict({\n",
        "    'train': ds['train'],\n",
        "    'test': ds_test['test'],\n",
        "    'valid': ds_test['train']\n",
        "})\n",
        "\n",
        "del ds_test\n",
        "\n",
        "print(prepared_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2191c214",
      "metadata": {
        "id": "2191c214"
      },
      "source": [
        "# Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849a73e7",
      "metadata": {
        "id": "849a73e7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, ViTForImageClassification\n",
        "import transformers\n",
        "class_labels = ['blackheads', 'dark spot', 'nodules', 'papules', 'pustules', 'whiteheads']\n",
        "\n",
        "def transform(example_batch):\n",
        "    desired_size = (224, 224)\n",
        "    images = []\n",
        "    root_dir = 'lump/images'\n",
        "    # Load and resize images\n",
        "    for img_path in example_batch['filename']:\n",
        "        img = Image.open(f\"{root_dir}/{img_path}\").convert(\"RGB\")\n",
        "        img_resized = transforms.Resize(desired_size)(img)\n",
        "        images.append(img_resized)\n",
        "\n",
        "    # Process images with the processor\n",
        "    inputs = processor(images, return_tensors='pt')\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    labels_batch = {k.strip(): example_batch[k.strip()] for k in example_batch.keys() if k.strip() in class_labels}\n",
        "    labels_matrix = torch.zeros((len(images), len(class_labels)))\n",
        "\n",
        "    for idx, label in enumerate(class_labels):\n",
        "        labels_matrix[:, idx] = torch.tensor(labels_batch[label])\n",
        "\n",
        "    # Add labels to the inputs\n",
        "    inputs['labels'] = labels_matrix\n",
        "    # print(inputs)\n",
        "    return inputs\n",
        "\n",
        "# Apply the transform function to the dataset\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-tiny-1k-224\")\n",
        "prepared_ds = ds.with_transform(transform)\n",
        "print(prepared_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    pixel_values = torch.stack([x['pixel_values'] for x in batch])\n",
        "    # print(pixel_values)\n",
        "    # print([x['labels'] for x in batch])\n",
        "    labels = torch.stack([x['labels'] for x in batch])\n",
        "    return {\n",
        "        'pixel_values': pixel_values,\n",
        "        'labels': labels\n",
        "    }"
      ],
      "metadata": {
        "id": "IDmW9NMyfX_-"
      },
      "id": "IDmW9NMyfX_-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9244ed15",
      "metadata": {
        "id": "9244ed15"
      },
      "source": [
        "# Model Training and Saving\n",
        "Adjust num_train_epochs in Training Arguments to higher value for more fit.\n",
        "Note: save_model_to_bestmodels does not evaluate accuracy, it only saves the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c80c4121",
      "metadata": {
        "id": "c80c4121"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "import os\n",
        "\n",
        "cl = ['blackheads', 'dark_spot', 'nodules', 'papules', 'pustules', 'whiteheads']\n",
        "\n",
        "# Function to save models to 'bestmodels' directory\n",
        "def save_model_to_bestmodels(model_id, model_name):\n",
        "    print(f\"Training and evaluating model: {model_id}\")\n",
        "\n",
        "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "    model = AutoModelForImageClassification.from_pretrained(\n",
        "        model_id,\n",
        "        num_labels=6,\n",
        "        id2label={str(i): c for i, c in enumerate(cl)},\n",
        "        label2id={c: str(i) for i, c in enumerate(cl)},\n",
        "        ignore_mismatched_sizes=True,\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"bestmodels/{model_name}\",\n",
        "        per_device_train_batch_size=16,\n",
        "        evaluation_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        fp16=True,\n",
        "        num_train_epochs=1,\n",
        "        logging_steps=500,\n",
        "        learning_rate=2e-4,\n",
        "        save_total_limit=1,\n",
        "        remove_unused_columns=False,\n",
        "        push_to_hub=False,\n",
        "        report_to='tensorboard',\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=collate_fn,\n",
        "        train_dataset=prepared_ds[\"train\"],\n",
        "        tokenizer=processor,\n",
        "    )\n",
        "\n",
        "    train_results = trainer.train()\n",
        "    trainer.save_model(f\"bestmodels/{model_name}\")\n",
        "    print(f\"Model saved to 'bestmodels/{model_name}'\")\n",
        "\n",
        "# Create 'bestmodels' directory if it doesn't exist\n",
        "os.makedirs(\"bestmodels\", exist_ok=True)\n",
        "\n",
        "# Models to train and save\n",
        "model_list = [\n",
        "    (\"google/vit-base-patch16-224\", \"vit\"),\n",
        "    (\"openai/clip-vit-base-patch32\", \"clip\"),\n",
        "    (\"google/siglip-base-patch16-224\", \"siglip\"),\n",
        "    (\"facebook/convnextv2-tiny-1k-224\", \"convnext\"),\n",
        "    (\"apple/mobilevitv2-1.0-imagenet1k-256\", \"mobilevit\"),\n",
        "    (\"google/mobilenet_v1_1.0_224\", \"mobilenet\")\n",
        "]\n",
        "\n",
        "# Train and save each model\n",
        "for model_id, model_name in model_list:\n",
        "    save_model_to_bestmodels(model_id, model_name)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}