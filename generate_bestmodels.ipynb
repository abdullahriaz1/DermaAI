{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c915d368",
      "metadata": {
        "id": "c915d368"
      },
      "source": [
        "# Setup and Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffeeb0d5",
      "metadata": {
        "id": "ffeeb0d5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q datasets\n",
        "!pip install matplotlib\n",
        "!pip install -U transformers\n",
        "!pip install scikit-learn pillow torchvision opencv-python\n",
        "!pip install tensorboardX\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install -U accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BRIOi_Dsi5Q0",
      "metadata": {
        "id": "BRIOi_Dsi5Q0"
      },
      "source": [
        "# In Google Colab, use /content folder for uploading lump.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062f2e88",
      "metadata": {
        "id": "062f2e88"
      },
      "outputs": [],
      "source": [
        "!unzip lump.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0107a42",
      "metadata": {
        "id": "e0107a42"
      },
      "source": [
        "# Data Loading and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bfc533",
      "metadata": {
        "id": "56bfc533"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load and prepare dataset\n",
        "csv_file = 'lump/merged.csv'  # Modify this path as needed\n",
        "df = pd.read_csv(csv_file)\n",
        "df.columns = df.columns.str.strip()\n",
        "print(\"Columns in the DataFrame:\", df.columns)\n",
        "\n",
        "# Define classes for acne classification\n",
        "class_labels = ['blackheads', 'dark_spot', 'nodules', 'papules', 'pustules', 'whiteheads']\n",
        "\n",
        "# Convert DataFrame to Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "ds = dataset.train_test_split(test_size=0.3)  # 70% train, 30% test\n",
        "ds_test = ds['test'].train_test_split(test_size=0.5)  # 30% test --> 15% valid, 15% test\n",
        "\n",
        "prepared_ds = DatasetDict({\n",
        "    'train': ds['train'],\n",
        "    'test': ds_test['test'],\n",
        "    'valid': ds_test['train']\n",
        "})\n",
        "\n",
        "del ds_test\n",
        "\n",
        "print(prepared_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2191c214",
      "metadata": {
        "id": "2191c214"
      },
      "source": [
        "# Data Transformation\n",
        "\n",
        "Must run this step with a provided processor before proceeding with Model Training and Saving!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849a73e7",
      "metadata": {
        "id": "849a73e7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, ViTForImageClassification\n",
        "import transformers\n",
        "\n",
        "# List of possible acne-related skin conditions\n",
        "class_labels = ['blackheads', 'dark spot', 'nodules', 'papules', 'pustules', 'whiteheads']\n",
        "\n",
        "def transform(example_batch):\n",
        "    # Define the desired image size\n",
        "    desired_size = (224, 224)\n",
        "    images = []\n",
        "    root_dir = 'lump/images'\n",
        "    \n",
        "    # Load and resize images\n",
        "    # The function goes through the filenames in example_batch and loads each image from the lump/images directory, \n",
        "    # converting it to RGB format (ensures 3 color channels).\n",
        "    for img_path in example_batch['filename']:\n",
        "        img = Image.open(f\"{root_dir}/{img_path}\").convert(\"RGB\")\n",
        "        img_resized = transforms.Resize(desired_size)(img)\n",
        "        images.append(img_resized)\n",
        "\n",
        "    # Process images with the processor to convert them into tensors\n",
        "    # processor (from AutoImageProcessor) turns the list of images into tensors that can be fed into a neural network.\n",
        "    # return_tensors='pt' means it will return a PyTorch tensor.\n",
        "    inputs = processor(images, return_tensors='pt')\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    # The function gets the relevant labels from example_batch and creates a zero matrix (labels_matrix)\n",
        "    # with a shape of (number_of_images, number_of_classes).\n",
        "    labels_batch = {k.strip(): example_batch[k.strip()] for k in example_batch.keys() if k.strip() in class_labels}\n",
        "    labels_matrix = torch.zeros((len(images), len(class_labels)))\n",
        "\n",
        "    # Each label is one-hot encoded by setting the correct positions in the matrix to 1 based on the input\n",
        "    for idx, label in enumerate(class_labels):\n",
        "        labels_matrix[:, idx] = torch.tensor(labels_batch[label])\n",
        "\n",
        "    # Add labels to the inputs\n",
        "    # The labels are added to the inputs dictionary, which has the image tensors, making it easy to use for training.\n",
        "    inputs['labels'] = labels_matrix\n",
        "    \n",
        "    # Return inputs, which now includes both processed images and one-hot encoded labels\n",
        "    return inputs\n",
        "\n",
        "# Load a pre-trained image processor from the transformers library\n",
        "# processor is created using the AutoImageProcessor class, loading a pre-trained model (\"facebook/convnextv2-tiny-1k-224\") \n",
        "# that helps prepare images in the right format for classification.\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-tiny-1k-224\")\n",
        "\n",
        "# Apply the transform function to the dataset, so each batch is processed on the fly\n",
        "# prepared_ds applies the transform function to an existing dataset ds using the .with_transform() method.\n",
        "# This means whenever the dataset is accessed, the transform function will process the data on the fly for training or testing.\n",
        "prepared_ds = ds.with_transform(transform)\n",
        "\n",
        "# Print the transformed dataset to check the output\n",
        "print(prepared_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IDmW9NMyfX_-",
      "metadata": {
        "id": "IDmW9NMyfX_-"
      },
      "outputs": [],
      "source": [
        "# This function combines individual image and label samples into a single batch for training or inference.\n",
        "def collate_fn(batch):\n",
        "    # Stack the 'pixel_values' from each item in the batch to create a single tensor.\n",
        "    # This results in a 4D tensor (batch_size, channels, height, width), where each entry is an image.\n",
        "    pixel_values = torch.stack([x['pixel_values'] for x in batch])\n",
        "    \n",
        "    # Stack the 'labels' from each item in the batch to create a single tensor.\n",
        "    # This results in a 2D tensor (batch_size, number_of_classes), where each entry is a one-hot encoded label.\n",
        "    labels = torch.stack([x['labels'] for x in batch])\n",
        "    \n",
        "    # Return a dictionary containing the batched pixel values and labels.\n",
        "    # This format is suitable for training or inference, where the model receives the inputs in batch.\n",
        "    return {\n",
        "        'pixel_values': pixel_values,\n",
        "        'labels': labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9244ed15",
      "metadata": {
        "id": "9244ed15"
      },
      "source": [
        "# Model Training and Saving\n",
        "Adjust num_train_epochs in Training Arguments to higher value for more fit.\n",
        "Note: save_model_to_bestmodels does not evaluate accuracy, it only saves the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c80c4121",
      "metadata": {
        "id": "c80c4121"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "import os\n",
        "\n",
        "cl = ['blackheads', 'dark_spot', 'nodules', 'papules', 'pustules', 'whiteheads']\n",
        "\n",
        "# Function to train a model and save it to the 'bestmodels' directory\n",
        "def save_model_to_bestmodels(model_id, model_name):\n",
        "    print(f\"Training and evaluating model: {model_id}\")\n",
        "\n",
        "    # Load a pre-trained image processor to handle image inputs\n",
        "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "    \n",
        "    # Load a pre-trained model for image classification, adjusted for multi-label classification\n",
        "    model = AutoModelForImageClassification.from_pretrained(\n",
        "        model_id,\n",
        "        num_labels=6,  # Set to classify 6 different labels\n",
        "        id2label={str(i): c for i, c in enumerate(cl)},  # Map indices to class labels\n",
        "        label2id={c: str(i) for i, c in enumerate(cl)},  # Map class labels to indices\n",
        "        ignore_mismatched_sizes=True,  # Allow flexibility with input size\n",
        "        problem_type=\"multi_label_classification\"  # Specify that this is a multi-label problem\n",
        "    )\n",
        "\n",
        "    # Set up training arguments to configure how the model will be trained\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"bestmodels/{model_name}\",  # Directory to save the model\n",
        "        per_device_train_batch_size=16,  # Batch size for training\n",
        "        evaluation_strategy=\"no\",  # No evaluation, this function only trains!\n",
        "        save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
        "        fp16=True,  # Use mixed-precision training for better performance, only available with GPU!\n",
        "        num_train_epochs=1,  # Train for 1 epoch\n",
        "        logging_steps=500,  # Log progress every 500 steps\n",
        "        learning_rate=2e-4,  # Set learning rate for optimization\n",
        "        save_total_limit=1,  # Keep only the latest model saved\n",
        "        remove_unused_columns=False,  # Retain all data features during training\n",
        "        push_to_hub=False,  # Do not push the model to Hugging Face Hub\n",
        "        report_to='tensorboard',  # Send training logs to TensorBoard\n",
        "    )\n",
        "\n",
        "    # Set up the Trainer to manage the training process\n",
        "    trainer = Trainer(\n",
        "        model=model,  # The model to train\n",
        "        args=training_args,  # Training configuration\n",
        "        data_collator=collate_fn,  # Handle batch processing\n",
        "        train_dataset=prepared_ds[\"train\"],  # The training dataset\n",
        "        tokenizer=processor,  # Use the processor for image handling\n",
        "    )\n",
        "\n",
        "    # Start training the model\n",
        "    train_results = trainer.train()\n",
        "    \n",
        "    # Save the trained model to the specified directory\n",
        "    trainer.save_model(f\"bestmodels/{model_name}\")\n",
        "    \n",
        "    # Print a message confirming the model has been saved\n",
        "    print(f\"Model saved to 'bestmodels/{model_name}'\")\n",
        "\n",
        "\n",
        "# Create 'bestmodels' directory if it doesn't exist\n",
        "os.makedirs(\"bestmodels\", exist_ok=True)\n",
        "\n",
        "# Models to train and save\n",
        "model_list = [\n",
        "    (\"google/vit-base-patch16-224\", \"vit\"),\n",
        "    (\"openai/clip-vit-base-patch32\", \"clip\"),\n",
        "    (\"google/siglip-base-patch16-224\", \"siglip\"),\n",
        "    (\"facebook/convnextv2-tiny-1k-224\", \"convnext\"),\n",
        "    (\"apple/mobilevitv2-1.0-imagenet1k-256\", \"mobilevit\"),\n",
        "    (\"google/mobilenet_v1_1.0_224\", \"mobilenet\")\n",
        "]\n",
        "\n",
        "# Train and save each model\n",
        "for model_id, model_name in model_list:\n",
        "    save_model_to_bestmodels(model_id, model_name)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
